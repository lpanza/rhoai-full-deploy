---
# Source: nvidia-operator-config/templates/rbac/service-accout.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: null
  name: rhoai-gpu-updater
  namespace: nvidia-gpu-operator
---
# Source: nvidia-operator-config/templates/rbac/role-view-pod.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: view-pod
  namespace: nvidia-gpu-operator
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
---
# Source: nvidia-operator-config/templates/rbac/role-binding-view-pod.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: vew-pod-rhoai-gpu-updater-sa
  namespace: nvidia-gpu-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: view-pod
subjects:
- kind: ServiceAccount
  name: rhoai-gpu-updater
  namespace: nvidia-gpu-operator
---
# Source: nvidia-operator-config/templates/rbac/job-rhoai-gpu-updater.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: rhoai-gpu-updater
  annotations:
    argocd.argoproj.io/hook: PostSync
spec:
  template:
    spec:
      restartPolicy: OnFailure
      serviceAccountName: rhoai-gpu-updater
      containers:
      - image: image-registry.openshift-image-registry.svc:5000/openshift/cli:latest
        name: rhoai-gpu-updater
        command:
        - /bin/bash
        args:
        - -c
        - |
          while true
          do
            echo "Checking if nvidia-operator-validator pod exists..."

            oc get pods -n nvidia-gpu-operator | grep nvidia-operator-validator
            if [[ $? -eq '0' ]]; then
              echo "Found! Breaking loop"
              break
            fi
            sleep 5s
          done

          export VALIDATOR_POD=`oc get pods -n nvidia-gpu-operator | grep nvidia-operator-validator | awk '{ print $1 }'`

          echo "Waiting for ${VALIDATOR_POD} to be Ready..."

          oc wait -n nvidia-gpu-operator --for=condition=Ready pod/${VALIDATOR_POD} --timeout=-1s

          echo "${VALIDATOR_POD} Ready, deleting migration-gpu-status cm and restarting dashboard Pods"

          oc delete cm -n redhat-ods-applications migration-gpu-status
          oc delete pods -n redhat-ods-applications -l app.opendatahub.io/rhods-dashboard=true
---
# Source: nvidia-operator-config/templates/gpu-cluster-policy.yaml
kind: ClusterPolicy
apiVersion: nvidia.com/v1
metadata:
  name: gpu-cluster-policy
  annotations:
    argocd.argoproj.io/sync-wave: "75"
spec:
  operator:
    defaultRuntime: crio
    use_ocp_driver_toolkit: true
    initContainer: {}
  sandboxWorkloads:
    enabled: false
    defaultWorkload: container
  driver:
    enabled: true
    useNvidiaDriverCRD: false
    useOpenKernelModules: false
    upgradePolicy:
      autoUpgrade: true
      drain:
        deleteEmptyDir: false
        enable: false
        force: false
        timeoutSeconds: 300
      maxParallelUpgrades: 1
      maxUnavailable: 25%
      podDeletion:
        deleteEmptyDir: false
        force: false
        timeoutSeconds: 300
      waitForCompletion:
        timeoutSeconds: 0
    repoConfig:
      configMapName: ''
    certConfig:
      name: ''
    licensingConfig:
      nlsEnabled: true
      configMapName: ''
    virtualTopology:
      config: ''
    kernelModuleConfig:
      name: ''
  dcgmExporter:
    enabled: true
    config:
      name: ''
    serviceMonitor:
      enabled: true
  dcgm:
    enabled: true
  daemonsets:
    updateStrategy: RollingUpdate
    rollingUpdate:
      maxUnavailable: '1'
  devicePlugin:
    enabled: true
    config:
      name: ''
      default: ''
  gfd:
    enabled: true
  migManager:
    enabled: true
  nodeStatusExporter:
    enabled: true
  mig:
    strategy: single
  toolkit:
    enabled: true
  validator:
    plugin:
      env:
        - name: WITH_WORKLOAD
          value: 'false'
  vgpuManager:
    enabled: false
  vgpuDeviceManager:
    enabled: true
  sandboxDevicePlugin:
    enabled: true
  vfioManager:
    enabled: true
  gds:
    enabled: false
